{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Text bloc detection à partir du PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "import pdfminer\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a PDF file.\n",
    "fp = open('<resume name here>.pdf', 'rb')\n",
    "\n",
    "# Create a PDF parser object associated with the file object.\n",
    "parser = PDFParser(fp)\n",
    "\n",
    "# Create a PDF document object that stores the document structure.\n",
    "# Password for initialization as 2nd parameter\n",
    "document = PDFDocument(parser)\n",
    "\n",
    "# Check if the document allows text extraction. If not, abort.\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "\n",
    "# Create a PDF resource manager object that stores shared resources.\n",
    "rsrcmgr = PDFResourceManager()\n",
    "\n",
    "# Create a PDF device object.\n",
    "device = PDFDevice(rsrcmgr)\n",
    "\n",
    "# BEGIN LAYOUT ANALYSIS\n",
    "# Set parameters for analysis.\n",
    "laparams = LAParams()\n",
    "\n",
    "# Create a PDF page aggregator object.\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "# Create a PDF interpreter object.\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "df = pd.DataFrame( columns = ['x', 'y','value']) \n",
    "def parse_obj(lt_objs,df):\n",
    "\n",
    "    # loop over the object list\n",
    "    for obj in lt_objs:\n",
    "\n",
    "        # if it's a textbox, print text and location\n",
    "        if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            print (\"%6d, %6d, %s\" % (obj.bbox[0], obj.bbox[1], obj.get_text().replace('\\n', '_')))\n",
    "            #new_row = {'x':obj.bbox[0], 'y':obj.bbox[1], 'value':obj.get_text().replace('\\n', '_')}\n",
    "            new_row = {'x':obj.bbox[0], 'y':obj.bbox[1], 'value':obj.get_text()}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # if it's a container, recurse\n",
    "        elif isinstance(obj, pdfminer.layout.LTFigure):\n",
    "            parse_obj(obj._objs,df)\n",
    "            \n",
    "    return df    \n",
    "\n",
    "# loop over all pages in the document\n",
    "for page in PDFPage.create_pages(document):\n",
    "\n",
    "    # read the page into a layout object\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "\n",
    "    # extract text from this object\n",
    "    df=parse_obj(layout._objs,df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bloc']=''\n",
    "df['bloc'] = np.where(df['x'].between(0,100), 'L', df['bloc'])\n",
    "\n",
    "df['bloc'] = np.where(df['x'].between(101,500), 'R', df['bloc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = df[df['bloc'] == 'L']\n",
    "df_right = df[df['bloc'] == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left=df_left.sort_values(\"y\", ascending=False)\n",
    "df_right=df_right.sort_values(\"y\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left['value'] = df_left['value'].map(lambda x: x.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right['value'] = df_right['value'].map(lambda x: ' '+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right = df_right.append(df_right).reset_index().drop_duplicates(subset='index').drop(columns='index')\n",
    "df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Détection des données à caractère personnel + Anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import fr_core_news_lg\n",
    "from spacy.matcher import Matcher #Adding custom entities\n",
    "from spacy.tokens import Span #Adding custom entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_lg.load()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_email(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EMAIL\")\n",
    "    doc.ents += (entity,)\n",
    "    \n",
    "def add_url(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"URL\")\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "def add_tel(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"TEL\")\n",
    "    doc.ents += (entity,)\n",
    "    \n",
    "def add_date(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"DATE\")\n",
    "    doc.ents += (entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\"URL\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "          \"EMAIL\": \"RGB(241, 13, 105)\",\n",
    "          \"TEL\" : \"RGB(95, 205, 231)\",\n",
    "          \"LOC\" : \"RGB(32, 240, 171)\",\n",
    "          \"PER\" : \"RGB(200, 232, 40)\"\n",
    "         }\n",
    "options = { \"colors\": colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternMail = [{\"LIKE_EMAIL\": True}]\n",
    "patternUrl = [{\"LIKE_URL\": True}]\n",
    "patternTel = [{\"LIKE_NUM\": True},{\"LENGTH\": {\">\": 4}}]\n",
    "patternDate = [{\"LIKE_NUM\": True},{\"LENGTH\": {\"==\": 4}}]\n",
    "\n",
    "matcher.add(\"mail\", add_email, patternMail)\n",
    "matcher.add(\"url\", add_url, patternUrl)\n",
    "matcher.add(\"tel\", add_tel, patternTel)\n",
    "matcher.add(\"date\", add_date, patternDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "df_right['Anonym']=spacy.tokens.doc.Doc\n",
    "\n",
    "for i in df_right['value']:\n",
    "    doc = nlp(i)\n",
    "    matches = matcher(doc)\n",
    "    displacy.render(doc, style=\"ent\", options=options)\n",
    "    df_right['Anonym'][count]=doc\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_person_names(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'PER':\n",
    "        return '[REDACTED_PER] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'LOC':\n",
    "        return '[REDACTED_LOC] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'EMAIL':\n",
    "        return '[REDACTEDE_MAIL] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'TEL':\n",
    "        return '[REDACTED_TEL] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'URL':\n",
    "        return '[REDACTED_URL] '\n",
    "   \n",
    "    return token.string\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    if  nlp_doc == spacy.tokens.doc.Doc:\n",
    "        return nlp_doc.text\n",
    "    else :\n",
    "        for ent in nlp_doc.ents:        \n",
    "            ent.merge()\n",
    "        tokens = map(replace_person_names, nlp_doc)\n",
    "        return ''.join(tokens)\n",
    "        \n",
    "    \n",
    "\n",
    "for i in df_right['Anonym']:\n",
    "    print(redact_names(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}