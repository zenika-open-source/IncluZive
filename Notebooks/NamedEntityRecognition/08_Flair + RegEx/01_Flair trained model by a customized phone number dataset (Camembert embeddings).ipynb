{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.data import Sentence\n",
    "from typing import List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flair\n",
    "from flair.embeddings import CamembertEmbeddings, FlairEmbeddings, TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Vous</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>pouvez</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>me</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>contacter</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>sur</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>mon</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>numéro</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>portable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>698768760</td>\n",
       "      <td>S-TEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Mon</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>numéro</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>portable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>est</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>003307653432123</td>\n",
       "      <td>S-TEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>Pour</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>tout</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>renseignement</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 3</td>\n",
       "      <td>je</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence             Word    Tag\n",
       "0   Sentence: 1             Vous      O\n",
       "1   Sentence: 1           pouvez      O\n",
       "2   Sentence: 1               me      O\n",
       "3   Sentence: 1        contacter      O\n",
       "4   Sentence: 1              sur      O\n",
       "5   Sentence: 1              mon      O\n",
       "6   Sentence: 1           numéro      O\n",
       "7   Sentence: 1         portable      O\n",
       "8   Sentence: 1        698768760  S-TEL\n",
       "9   Sentence: 1                .      O\n",
       "10  Sentence: 2              Mon      O\n",
       "11  Sentence: 2           numéro      O\n",
       "12  Sentence: 2         portable      O\n",
       "13  Sentence: 2              est      O\n",
       "14  Sentence: 2  003307653432123  S-TEL\n",
       "15  Sentence: 2                .      O\n",
       "16  Sentence: 3             Pour      O\n",
       "17  Sentence: 3             tout      O\n",
       "18  Sentence: 3    renseignement      O\n",
       "19  Sentence: 3               je      O"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Training_dataset_phonenumber.csv', encoding='latin1')\n",
    "data = data.fillna(method='ffill')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_getter(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        agg_fun = lambda s: [(w, t) for w,t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        \n",
    "        self.grouped = self.data.groupby('Sentence').apply(agg_fun)\n",
    "        self.sentences = [i for i in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = Sentence_getter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vous', 'O'),\n",
       " ('pouvez', 'O'),\n",
       " ('me', 'O'),\n",
       " ('contacter', 'O'),\n",
       " ('sur', 'O'),\n",
       " ('mon', 'O'),\n",
       " ('numéro', 'O'),\n",
       " ('portable', 'O'),\n",
       " ('698768760', 'S-TEL'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous pouvez me contacter sur mon numéro portable 698768760 . \n",
      "Sentence: \"Vous pouvez me contacter sur mon numéro portable 698768760 .\"   [− Tokens: 10]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mon nouveau numéro portable est 6543456787 . \n",
      "Sentence: \"Mon nouveau numéro portable est 6543456787 .\"   [− Tokens: 7]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3454323456 est mon ancien numéro portable . \n",
      "Sentence: \"3454323456 est mon ancien numéro portable .\"   [− Tokens: 7]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le numéro 76543456787 n n est pas correct . \n",
      "Sentence: \"Le numéro 76543456787 n n est pas correct .\"   [− Tokens: 9]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le numéro 98765456787 n n est pas valide . \n",
      "Sentence: \"Le numéro 98765456787 n n est pas valide .\"   [− Tokens: 9]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ce numéro 3434234567 est erroné . \n",
      "Sentence: \"Ce numéro 3434234567 est erroné .\"   [− Tokens: 6]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le numéro auquel on devrait envoyer notre demande est 76545676543 . \n",
      "Sentence: \"Le numéro auquel on devrait envoyer notre demande est 76545676543 .\"   [− Tokens: 11]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mon numéro portable est 003307653432123 . \n",
      "Sentence: \"Mon numéro portable est 003307653432123 .\"   [− Tokens: 6]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pour tout renseignement je suis joignable sur le numéro portable 0021699987987 . \n",
      "Sentence: \"Pour tout renseignement je suis joignable sur le numéro portable 0021699987987 .\"   [− Tokens: 12]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 099767767 est mon numéro portable . \n",
      "Sentence: \"099767767 est mon numéro portable .\"   [− Tokens: 6]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "On peut contacter le consultant via son numéro portable 1212123456 . \n",
      "Sentence: \"On peut contacter le consultant via son numéro portable 1212123456 .\"   [− Tokens: 11]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "On devrait utiliser ce numéro 765678765 . \n",
      "Sentence: \"On devrait utiliser ce numéro 765678765 .\"   [− Tokens: 7]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Son numéro portable est 9876567898 . \n",
      "Sentence: \"Son numéro portable est 9876567898 .\"   [− Tokens: 6]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "On devrait envoyer la reclamation à ce numéro portable 1234123412 . \n",
      "Sentence: \"On devrait envoyer la reclamation à ce numéro portable 1234123412 .\"   [− Tokens: 11]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pour contacter le client il faut envoyer un sms au 5676543456 . \n",
      "Sentence: \"Pour contacter le client il faut envoyer un sms au 5676543456 .\"   [− Tokens: 12]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged = list()\n",
    "listOfSent =list()   \n",
    "strr=\"\"\n",
    "for a in range(len(sentences)):\n",
    "\n",
    "    for word in sentences[a] :\n",
    "        strr = strr + word[0]+ \" \"\n",
    "        tagged.append(word[1])\n",
    "        \n",
    "    sent = Sentence(strr)\n",
    "    print(strr)\n",
    "    print(sent)\n",
    "    for x in range(len(sent)):\n",
    "        sent[x].add_tag('ner', tagged[x])\n",
    "        print('')\n",
    "        \n",
    "    strr =\"\"\n",
    "    tagged=[]  \n",
    "    listOfSent.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = listOfSent[:10]\n",
    "dev = listOfSent[:2]\n",
    "test= listOfSent[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<unk>', b'O', b'S-TEL', b'<START>', b'<STOP>']\n",
      "2020-12-22 20:12:00,646 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,649 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:12:00,650 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,651 Corpus: \"Corpus: 10 train + 2 dev + 5 test sentences\"\n",
      "2020-12-22 20:12:00,652 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,654 Parameters:\n",
      "2020-12-22 20:12:00,655  - learning_rate: \"0.1\"\n",
      "2020-12-22 20:12:00,656  - mini_batch_size: \"32\"\n",
      "2020-12-22 20:12:00,656  - patience: \"3\"\n",
      "2020-12-22 20:12:00,657  - anneal_factor: \"0.5\"\n",
      "2020-12-22 20:12:00,658  - max_epochs: \"30\"\n",
      "2020-12-22 20:12:00,658  - shuffle: \"True\"\n",
      "2020-12-22 20:12:00,659  - train_with_dev: \"False\"\n",
      "2020-12-22 20:12:00,660  - batch_growth_annealing: \"False\"\n",
      "2020-12-22 20:12:00,661 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,661 Model training base path: \"resources/taggers/flair+camembertEmbeddings\"\n",
      "2020-12-22 20:12:00,662 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,662 Device: cpu\n",
      "2020-12-22 20:12:00,663 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:00,664 Embeddings storage mode: cpu\n",
      "2020-12-22 20:12:00,669 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:01,790 epoch 1 - iter 1/1 - loss 16.94610214 - samples/sec: 28.60 - lr: 0.100000\n",
      "2020-12-22 20:12:01,791 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:01,792 EPOCH 1 done: loss 16.9461 - lr 0.1000000\n",
      "2020-12-22 20:12:01,830 DEV : loss 6.702705383300781 - score 0.7647\n",
      "2020-12-22 20:12:01,831 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:03,059 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:03,324 epoch 2 - iter 1/1 - loss 7.45570087 - samples/sec: 121.30 - lr: 0.100000\n",
      "2020-12-22 20:12:03,326 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:03,328 EPOCH 2 done: loss 7.4557 - lr 0.1000000\n",
      "2020-12-22 20:12:03,375 DEV : loss 4.617948055267334 - score 0.8824\n",
      "2020-12-22 20:12:03,376 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:04,752 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:04,984 epoch 3 - iter 1/1 - loss 4.63498926 - samples/sec: 138.32 - lr: 0.100000\n",
      "2020-12-22 20:12:04,986 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:04,987 EPOCH 3 done: loss 4.6350 - lr 0.1000000\n",
      "2020-12-22 20:12:05,021 DEV : loss 3.2725181579589844 - score 0.7647\n",
      "2020-12-22 20:12:05,022 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:05,582 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:05,837 epoch 4 - iter 1/1 - loss 3.38247538 - samples/sec: 125.74 - lr: 0.100000\n",
      "2020-12-22 20:12:05,839 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:05,839 EPOCH 4 done: loss 3.3825 - lr 0.1000000\n",
      "2020-12-22 20:12:05,874 DEV : loss 3.773951530456543 - score 0.8824\n",
      "2020-12-22 20:12:05,875 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:07,122 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:07,340 epoch 5 - iter 1/1 - loss 3.57389140 - samples/sec: 147.65 - lr: 0.100000\n",
      "2020-12-22 20:12:07,341 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:07,342 EPOCH 5 done: loss 3.5739 - lr 0.1000000\n",
      "2020-12-22 20:12:07,376 DEV : loss 1.9616851806640625 - score 1.0\n",
      "2020-12-22 20:12:07,378 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:08,554 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:08,800 epoch 6 - iter 1/1 - loss 2.24782372 - samples/sec: 130.59 - lr: 0.100000\n",
      "2020-12-22 20:12:08,801 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:08,802 EPOCH 6 done: loss 2.2478 - lr 0.1000000\n",
      "2020-12-22 20:12:08,833 DEV : loss 2.2102136611938477 - score 0.8824\n",
      "2020-12-22 20:12:08,834 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:09,542 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:09,814 epoch 7 - iter 1/1 - loss 2.21727347 - samples/sec: 118.45 - lr: 0.100000\n",
      "2020-12-22 20:12:09,816 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:09,818 EPOCH 7 done: loss 2.2173 - lr 0.1000000\n",
      "2020-12-22 20:12:09,863 DEV : loss 0.7645673751831055 - score 1.0\n",
      "2020-12-22 20:12:09,864 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:11,325 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:11,651 epoch 8 - iter 1/1 - loss 1.07443464 - samples/sec: 98.59 - lr: 0.100000\n",
      "2020-12-22 20:12:11,652 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:11,653 EPOCH 8 done: loss 1.0744 - lr 0.1000000\n",
      "2020-12-22 20:12:11,685 DEV : loss 0.5238065719604492 - score 1.0\n",
      "2020-12-22 20:12:11,687 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:13,170 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:13,520 epoch 9 - iter 1/1 - loss 1.10541189 - samples/sec: 91.88 - lr: 0.100000\n",
      "2020-12-22 20:12:13,521 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:13,523 EPOCH 9 done: loss 1.1054 - lr 0.1000000\n",
      "2020-12-22 20:12:13,568 DEV : loss 0.5019102096557617 - score 1.0\n",
      "2020-12-22 20:12:13,570 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:15,760 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:16,458 epoch 10 - iter 1/1 - loss 0.59486246 - samples/sec: 46.13 - lr: 0.100000\n",
      "2020-12-22 20:12:16,460 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:16,462 EPOCH 10 done: loss 0.5949 - lr 0.1000000\n",
      "2020-12-22 20:12:16,528 DEV : loss 0.24652481079101562 - score 1.0\n",
      "2020-12-22 20:12:16,531 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:18,325 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:18,708 epoch 11 - iter 1/1 - loss 1.05653667 - samples/sec: 84.05 - lr: 0.100000\n",
      "2020-12-22 20:12:18,710 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:18,712 EPOCH 11 done: loss 1.0565 - lr 0.1000000\n",
      "2020-12-22 20:12:18,771 DEV : loss 0.18828868865966797 - score 1.0\n",
      "2020-12-22 20:12:18,777 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:20,946 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:21,359 epoch 12 - iter 1/1 - loss 0.35323983 - samples/sec: 78.09 - lr: 0.100000\n",
      "2020-12-22 20:12:21,360 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:21,361 EPOCH 12 done: loss 0.3532 - lr 0.1000000\n",
      "2020-12-22 20:12:21,419 DEV : loss 0.06882286071777344 - score 1.0\n",
      "2020-12-22 20:12:21,421 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:23,032 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:23,352 epoch 13 - iter 1/1 - loss 0.38492432 - samples/sec: 100.49 - lr: 0.100000\n",
      "2020-12-22 20:12:23,353 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:12:23,372 EPOCH 13 done: loss 0.3849 - lr 0.1000000\n",
      "2020-12-22 20:12:23,443 DEV : loss 0.021999359130859375 - score 1.0\n",
      "2020-12-22 20:12:23,444 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:25,144 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:25,474 epoch 14 - iter 1/1 - loss 0.08531799 - samples/sec: 97.43 - lr: 0.100000\n",
      "2020-12-22 20:12:25,476 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:25,478 EPOCH 14 done: loss 0.0853 - lr 0.1000000\n",
      "2020-12-22 20:12:25,532 DEV : loss 0.018497467041015625 - score 1.0\n",
      "2020-12-22 20:12:25,533 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:27,039 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:27,317 epoch 15 - iter 1/1 - loss 0.11406364 - samples/sec: 116.31 - lr: 0.100000\n",
      "2020-12-22 20:12:27,318 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:27,319 EPOCH 15 done: loss 0.1141 - lr 0.1000000\n",
      "2020-12-22 20:12:27,361 DEV : loss 0.016355514526367188 - score 1.0\n",
      "2020-12-22 20:12:27,363 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:28,664 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:28,970 epoch 16 - iter 1/1 - loss 0.04297485 - samples/sec: 105.05 - lr: 0.100000\n",
      "2020-12-22 20:12:28,971 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:28,973 EPOCH 16 done: loss 0.0430 - lr 0.1000000\n",
      "2020-12-22 20:12:29,014 DEV : loss 0.01354217529296875 - score 1.0\n",
      "2020-12-22 20:12:29,016 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:30,354 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:30,633 epoch 17 - iter 1/1 - loss 0.51498812 - samples/sec: 115.13 - lr: 0.100000\n",
      "2020-12-22 20:12:30,635 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:30,636 EPOCH 17 done: loss 0.5150 - lr 0.1000000\n",
      "2020-12-22 20:12:30,675 DEV : loss 0.0133514404296875 - score 1.0\n",
      "2020-12-22 20:12:30,677 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:32,299 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:32,598 epoch 18 - iter 1/1 - loss 0.31660405 - samples/sec: 107.44 - lr: 0.100000\n",
      "2020-12-22 20:12:32,600 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:32,601 EPOCH 18 done: loss 0.3166 - lr 0.1000000\n",
      "2020-12-22 20:12:32,653 DEV : loss 0.016357421875 - score 1.0\n",
      "2020-12-22 20:12:32,654 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:33,323 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:33,589 epoch 19 - iter 1/1 - loss 0.30119380 - samples/sec: 121.07 - lr: 0.100000\n",
      "2020-12-22 20:12:33,590 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:33,591 EPOCH 19 done: loss 0.3012 - lr 0.1000000\n",
      "2020-12-22 20:12:33,643 DEV : loss 0.009084701538085938 - score 1.0\n",
      "2020-12-22 20:12:33,653 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:34,847 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:35,122 epoch 20 - iter 1/1 - loss 0.08867206 - samples/sec: 117.07 - lr: 0.100000\n",
      "2020-12-22 20:12:35,123 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:35,124 EPOCH 20 done: loss 0.0887 - lr 0.1000000\n",
      "2020-12-22 20:12:35,159 DEV : loss 0.0073070526123046875 - score 1.0\n",
      "2020-12-22 20:12:35,161 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:36,334 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:36,584 epoch 21 - iter 1/1 - loss 0.04286728 - samples/sec: 128.55 - lr: 0.100000\n",
      "2020-12-22 20:12:36,585 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:36,586 EPOCH 21 done: loss 0.0429 - lr 0.1000000\n",
      "2020-12-22 20:12:36,622 DEV : loss 0.0067195892333984375 - score 1.0\n",
      "2020-12-22 20:12:36,623 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:37,883 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:38,157 epoch 22 - iter 1/1 - loss 0.46262074 - samples/sec: 117.85 - lr: 0.100000\n",
      "2020-12-22 20:12:38,159 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:38,160 EPOCH 22 done: loss 0.4626 - lr 0.1000000\n",
      "2020-12-22 20:12:38,201 DEV : loss 0.006824493408203125 - score 1.0\n",
      "2020-12-22 20:12:38,203 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:38,978 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:39,241 epoch 23 - iter 1/1 - loss 0.02562695 - samples/sec: 122.70 - lr: 0.100000\n",
      "2020-12-22 20:12:39,242 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:39,243 EPOCH 23 done: loss 0.0256 - lr 0.1000000\n",
      "2020-12-22 20:12:39,281 DEV : loss 0.006256103515625 - score 1.0\n",
      "2020-12-22 20:12:39,282 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:40,612 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:40,860 epoch 24 - iter 1/1 - loss 0.08289757 - samples/sec: 129.45 - lr: 0.100000\n",
      "2020-12-22 20:12:40,862 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:40,863 EPOCH 24 done: loss 0.0829 - lr 0.1000000\n",
      "2020-12-22 20:12:40,900 DEV : loss 0.005634307861328125 - score 1.0\n",
      "2020-12-22 20:12:40,901 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:42,092 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:42,339 epoch 25 - iter 1/1 - loss 0.03618927 - samples/sec: 130.63 - lr: 0.100000\n",
      "2020-12-22 20:12:42,340 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:42,341 EPOCH 25 done: loss 0.0362 - lr 0.1000000\n",
      "2020-12-22 20:12:42,377 DEV : loss 0.0053730010986328125 - score 1.0\n",
      "2020-12-22 20:12:42,379 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:43,528 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:43,778 epoch 26 - iter 1/1 - loss 0.04135780 - samples/sec: 128.47 - lr: 0.100000\n",
      "2020-12-22 20:12:43,779 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:43,780 EPOCH 26 done: loss 0.0414 - lr 0.1000000\n",
      "2020-12-22 20:12:43,822 DEV : loss 0.0051555633544921875 - score 1.0\n",
      "2020-12-22 20:12:43,848 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:44,976 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:45,225 epoch 27 - iter 1/1 - loss 0.39865723 - samples/sec: 129.08 - lr: 0.100000\n",
      "2020-12-22 20:12:45,226 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:45,227 EPOCH 27 done: loss 0.3987 - lr 0.1000000\n",
      "2020-12-22 20:12:45,268 DEV : loss 0.006137847900390625 - score 1.0\n",
      "2020-12-22 20:12:45,269 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:45,973 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:46,225 epoch 28 - iter 1/1 - loss 0.22627792 - samples/sec: 127.62 - lr: 0.100000\n",
      "2020-12-22 20:12:46,226 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:12:46,227 EPOCH 28 done: loss 0.2263 - lr 0.1000000\n",
      "2020-12-22 20:12:46,266 DEV : loss 0.0047454833984375 - score 1.0\n",
      "2020-12-22 20:12:46,266 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:47,434 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:47,684 epoch 29 - iter 1/1 - loss 0.17476597 - samples/sec: 128.63 - lr: 0.100000\n",
      "2020-12-22 20:12:47,685 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:47,686 EPOCH 29 done: loss 0.1748 - lr 0.1000000\n",
      "2020-12-22 20:12:47,723 DEV : loss 0.0045871734619140625 - score 1.0\n",
      "2020-12-22 20:12:47,724 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-22 20:12:48,910 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:49,175 epoch 30 - iter 1/1 - loss 0.48194543 - samples/sec: 121.29 - lr: 0.100000\n",
      "2020-12-22 20:12:49,176 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:49,176 EPOCH 30 done: loss 0.4819 - lr 0.1000000\n",
      "2020-12-22 20:12:49,212 DEV : loss 0.0052337646484375 - score 1.0\n",
      "2020-12-22 20:12:49,213 BAD EPOCHS (no improvement): 1\n",
      "2020-12-22 20:12:50,369 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:12:50,370 Testing using best model ...\n",
      "2020-12-22 20:12:50,371 loading file resources/taggers/flair+camembertEmbeddings/best-model.pt\n",
      "2020-12-22 20:12:52,981 \t0.9787\n",
      "2020-12-22 20:12:52,982 \n",
      "Results:\n",
      "- F-score (micro): 0.9787\n",
      "- F-score (macro): 0.9386\n",
      "- Accuracy (incl. no class): 0.9787\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.9767    1.0000    0.9882        42\n",
      "       S-TEL     1.0000    0.8000    0.8889         5\n",
      "\n",
      "    accuracy                         0.9787        47\n",
      "   macro avg     0.9884    0.9000    0.9386        47\n",
      "weighted avg     0.9792    0.9787    0.9777        47\n",
      "\n",
      "2020-12-22 20:12:52,983 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9787,\n",
       " 'dev_score_history': [0.7647,\n",
       "  0.8824,\n",
       "  0.7647,\n",
       "  0.8824,\n",
       "  1.0,\n",
       "  0.8824,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'train_loss_history': [16.946102142333984,\n",
       "  7.455700874328613,\n",
       "  4.634989261627197,\n",
       "  3.3824753761291504,\n",
       "  3.5738914012908936,\n",
       "  2.247823715209961,\n",
       "  2.217273473739624,\n",
       "  1.0744346380233765,\n",
       "  1.1054118871688843,\n",
       "  0.5948624610900879,\n",
       "  1.0565366744995117,\n",
       "  0.35323983430862427,\n",
       "  0.3849243223667145,\n",
       "  0.08531799167394638,\n",
       "  0.11406364291906357,\n",
       "  0.04297485202550888,\n",
       "  0.514988124370575,\n",
       "  0.31660404801368713,\n",
       "  0.30119380354881287,\n",
       "  0.08867206424474716,\n",
       "  0.042867280542850494,\n",
       "  0.46262073516845703,\n",
       "  0.02562694624066353,\n",
       "  0.08289756625890732,\n",
       "  0.03618926927447319,\n",
       "  0.04135780408978462,\n",
       "  0.3986572325229645,\n",
       "  0.2262779176235199,\n",
       "  0.17476597428321838,\n",
       "  0.4819454252719879],\n",
       " 'dev_loss_history': [6.702705383300781,\n",
       "  4.617948055267334,\n",
       "  3.2725181579589844,\n",
       "  3.773951530456543,\n",
       "  1.9616851806640625,\n",
       "  2.2102136611938477,\n",
       "  0.7645673751831055,\n",
       "  0.5238065719604492,\n",
       "  0.5019102096557617,\n",
       "  0.24652481079101562,\n",
       "  0.18828868865966797,\n",
       "  0.06882286071777344,\n",
       "  0.021999359130859375,\n",
       "  0.018497467041015625,\n",
       "  0.016355514526367188,\n",
       "  0.01354217529296875,\n",
       "  0.0133514404296875,\n",
       "  0.016357421875,\n",
       "  0.009084701538085938,\n",
       "  0.0073070526123046875,\n",
       "  0.0067195892333984375,\n",
       "  0.006824493408203125,\n",
       "  0.006256103515625,\n",
       "  0.005634307861328125,\n",
       "  0.0053730010986328125,\n",
       "  0.0051555633544921875,\n",
       "  0.006137847900390625,\n",
       "  0.0047454833984375,\n",
       "  0.0045871734619140625,\n",
       "  0.0052337646484375]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = Corpus(train, dev, test)\n",
    "\n",
    "# Make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=\"ner\")\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    TransformerWordEmbeddings('camembert-base')\n",
    "                                       ])\n",
    "\n",
    "# Initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=stacked_embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=\"ner\",\n",
    "                                        use_crf=True)\n",
    "\n",
    "# initialize trainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# Start training\n",
    "trainer.train('resources/taggers/flair+camembertEmbeddings',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=30,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:12:52,994 loading file resources/taggers/flair+camembertEmbeddings/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "model = SequenceTagger.load('resources/taggers/flair+camembertEmbeddings/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence('Le numéro de téléphone de Emma Louise est 0676765432. Elle était diplômée en 2019. Si ce numéro est injoignable, il faudra appeler 0612121212.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 21.4 ms, total: 217 ms\n",
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le numéro de téléphone de Emma Louise est 0676765432 <S-TEL> . Elle était diplômée en 2019 . Si ce numéro est injoignable , il faudra appeler 0612121212 <S-TEL> .\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[a.b] Fix me: Combine Flair Corpus with existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:46:38,878 Reading data from /Users/amalbedoui/.flair/datasets/wikiner_french\n",
      "2020-12-22 20:46:38,878 Train: /Users/amalbedoui/.flair/datasets/wikiner_french/aij-wikiner-fr-wp3.train\n",
      "2020-12-22 20:46:38,879 Dev: None\n",
      "2020-12-22 20:46:38,879 Test: None\n",
      "2020-12-22 20:46:44,372 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,375 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=5, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:46:44,376 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,377 Corpus: \"Corpus: 10713 train + 1190 dev + 1323 test sentences\"\n",
      "2020-12-22 20:46:44,377 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,378 Parameters:\n",
      "2020-12-22 20:46:44,378  - learning_rate: \"0.1\"\n",
      "2020-12-22 20:46:44,379  - mini_batch_size: \"32\"\n",
      "2020-12-22 20:46:44,380  - patience: \"3\"\n",
      "2020-12-22 20:46:44,380  - anneal_factor: \"0.5\"\n",
      "2020-12-22 20:46:44,381  - max_epochs: \"3\"\n",
      "2020-12-22 20:46:44,382  - shuffle: \"True\"\n",
      "2020-12-22 20:46:44,382  - train_with_dev: \"False\"\n",
      "2020-12-22 20:46:44,383  - batch_growth_annealing: \"False\"\n",
      "2020-12-22 20:46:44,384 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,384 Model training base path: \"resources/taggers/flair+camembertEmbeddings+flaircorpus\"\n",
      "2020-12-22 20:46:44,385 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,386 Device: cpu\n",
      "2020-12-22 20:46:44,387 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,387 Embeddings storage mode: cpu\n",
      "2020-12-22 20:46:44,919 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-22 20:46:44,920 Testing using best model ...\n",
      "2020-12-22 20:50:19,705 \t0.8776\n",
      "2020-12-22 20:50:19,707 \n",
      "Results:\n",
      "- F-score (micro): 0.8776\n",
      "- F-score (macro): 0.0521\n",
      "- Accuracy (incl. no class): 0.8776\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O     0.8864    0.9944    0.9373     31022\n",
      "       B-LOC     1.0000    0.0000    0.0000       279\n",
      "       I-LOC     1.0000    0.0000    0.0000       229\n",
      "       E-LOC     1.0000    0.0000    0.0000       279\n",
      "       S-TEL     0.0000    1.0000    0.0000         0\n",
      "       S-LOC     1.0000    0.0000    0.0000       869\n",
      "      B-MISC     1.0000    0.0000    0.0000       238\n",
      "      I-MISC     1.0000    0.0000    0.0000       219\n",
      "      E-MISC     1.0000    0.0000    0.0000       238\n",
      "      S-MISC     1.0000    0.0000    0.0000       168\n",
      "       B-PER     1.0000    0.0000    0.0000       406\n",
      "       E-PER     1.0000    0.0000    0.0000       406\n",
      "       S-ORG     1.0000    0.0000    0.0000       147\n",
      "       S-PER     1.0000    0.0000    0.0000       301\n",
      "       I-PER     1.0000    0.0000    0.0000        89\n",
      "       B-ORG     1.0000    0.0000    0.0000        91\n",
      "       I-ORG     1.0000    0.0000    0.0000        76\n",
      "       E-ORG     1.0000    0.0000    0.0000        91\n",
      "\n",
      "    accuracy                         0.8776     35148\n",
      "   macro avg     0.9381    0.1108    0.0521     35148\n",
      "weighted avg     0.8997    0.8776    0.8272     35148\n",
      "\n",
      "2020-12-22 20:50:19,708 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8776,\n",
       " 'dev_score_history': [],\n",
       " 'train_loss_history': [],\n",
       " 'dev_loss_history': []}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = 'resources/taggers/flair+camembertEmbeddings/checkpoint.pt'\n",
    "trainer = ModelTrainer.load_checkpoint(checkpoint, flair.datasets.WIKINER_FRENCH().downsample(0.1))\n",
    "trainer.train('resources/taggers/flair+camembertEmbeddings+flaircorpus',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=3,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-22 20:55:42,658 loading file resources/taggers/flair+camembertEmbeddings+flaircorpus/final-model.pt\n"
     ]
    }
   ],
   "source": [
    "newmodel = SequenceTagger.load('resources/taggers/flair+camembertEmbeddings+flaircorpus/final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsentence = Sentence('Emma Louise, habite au 26 rue Alexandre, 75005 Paris, France, née le 11/11/1993. Elle est F et travaille chez Zenika et elle est joignable sur 06660006.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 15.6 ms, total: 203 ms\n",
      "Wall time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "newmodel.predict(newsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Louise , habite au 26 <S-TEL> rue Alexandre , 75005 Paris , France , née le 11 <S-TEL> / 11 / 1993 . Elle est F et travaille chez Zenika et elle est joignable sur 06660006 <S-TEL> .\n"
     ]
    }
   ],
   "source": [
    "print(newsentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
