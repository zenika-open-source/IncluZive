{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Text bloc detection à partir du PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "import pdfminer\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a PDF file.\n",
    "fp = open('<cv-name-here>.pdf', 'rb')\n",
    "\n",
    "# Create a PDF parser object associated with the file object.\n",
    "parser = PDFParser(fp)\n",
    "\n",
    "# Create a PDF document object that stores the document structure.\n",
    "# Password for initialization as 2nd parameter\n",
    "document = PDFDocument(parser)\n",
    "\n",
    "# Check if the document allows text extraction. If not, abort.\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "\n",
    "# Create a PDF resource manager object that stores shared resources.\n",
    "rsrcmgr = PDFResourceManager()\n",
    "\n",
    "# Create a PDF device object.\n",
    "device = PDFDevice(rsrcmgr)\n",
    "\n",
    "# BEGIN LAYOUT ANALYSIS\n",
    "# Set parameters for analysis.\n",
    "laparams = LAParams()\n",
    "\n",
    "# Create a PDF page aggregator object.\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "# Create a PDF interpreter object.\n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "df = pd.DataFrame( columns = ['x', 'y','value']) \n",
    "def parse_obj(lt_objs,df):\n",
    "\n",
    "    # loop over the object list\n",
    "    for obj in lt_objs:\n",
    "\n",
    "        # if it's a textbox, print text and location\n",
    "        if isinstance(obj, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            print (\"%6d, %6d, %s\" % (obj.bbox[0], obj.bbox[1], obj.get_text().replace('\\n', '_')))\n",
    "            #new_row = {'x':obj.bbox[0], 'y':obj.bbox[1], 'value':obj.get_text().replace('\\n', '_')}\n",
    "            new_row = {'x':obj.bbox[0], 'y':obj.bbox[1], 'value':obj.get_text()}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        # if it's a container, recurse\n",
    "        elif isinstance(obj, pdfminer.layout.LTFigure):\n",
    "            parse_obj(obj._objs,df)\n",
    "            \n",
    "    return df    \n",
    "\n",
    "# loop over all pages in the document\n",
    "for page in PDFPage.create_pages(document):\n",
    "\n",
    "    # read the page into a layout object\n",
    "    interpreter.process_page(page)\n",
    "    layout = device.get_result()\n",
    "\n",
    "    # extract text from this object\n",
    "    df=parse_obj(layout._objs,df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bloc']=''\n",
    "df['bloc'] = np.where(df['x'].between(0,100), 'L', df['bloc'])\n",
    "\n",
    "df['bloc'] = np.where(df['x'].between(101,500), 'R', df['bloc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = df[df['bloc'] == 'L']\n",
    "df_right = df[df['bloc'] == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left=df_left.sort_values(\"y\", ascending=False)\n",
    "df_right=df_right.sort_values(\"y\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left['value'] = df_left['value'].map(lambda x: x.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right['value'] = df_right['value'].map(lambda x: ' '+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right = df_right.append(df_right).reset_index().drop_duplicates(subset='index').drop(columns='index')\n",
    "df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2: Détection des données à caractère personnel + Anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import fr_core_news_lg\n",
    "from prettytable import PrettyTable\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.matcher import Matcher #Adding custom entities\n",
    "from spacy.tokens import Span #Adding custom entities\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import re\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger,  MultiTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtre des POS de Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MultiTagger.load(['pos-multi', 'fr-ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_inclusive(str1,str2):\n",
    "    if str1.endswith(str2):\n",
    "        return str1[:-len(str2)]+\"(\"+str2+\")\"\n",
    "    else :\n",
    "        return str1+\"(\"+str2+\")\"\n",
    "   \n",
    "def add_inclusive_adj(str1,str2,str3):\n",
    "    if str1.endswith(str2):\n",
    "        return str1+\"(\"+str3+\")\"\n",
    "    elif str1.endswith(str3):\n",
    "        return str1[:-len(str3)]+str2+\"(\"+str3+\")\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_inclusive(sentence):\n",
    "    strr=\"\"\n",
    "    for entity in sentence.get_spans('pos-multi'):\n",
    "        \n",
    "        for data in entity.labels:\n",
    "            \n",
    "            if re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'VERB' and len(entity.text)>2 :\n",
    "                #[TO DO] Tester avec l'auxiliare qui précède le verbe: si avoir, pas de changement, sinon:\n",
    "                if entity.text.endswith(\"é\") or entity.text.endswith(\"e\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"e\")+\" \"\n",
    "                elif entity.text.endswith(\"i\") or entity.text.endswith(\"ie\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"ie\")+\" \"\n",
    "                elif entity.text.endswith(\"is\") or entity.text.endswith(\"ise\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"ise\")+\" \"\n",
    "                elif entity.text.endswith(\"t\") or entity.text.endswith(\"te\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"te\")+\" \"\n",
    "                elif entity.text.endswith(\"us\") or entity.text.endswith(\"use\"): #inclus(se)\n",
    "                    strr= strr+add_inclusive(entity.text,\"use\")+\" \"\n",
    "                elif entity.text.endswith(\"u\") or entity.text.endswith(\"ue\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"ue\")+\" \"\n",
    "                else :\n",
    "                    strr= strr+entity.text+\" \"\n",
    "  \n",
    "            elif re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'VERB' and len(entity.text)<2 :\n",
    "                strr= strr+entity.text+\" \"\n",
    "        \n",
    "        \n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'ADJ'  :\n",
    "                if entity.text.endswith(\"ien\") or entity.text.endswith(\"ienne\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"ien\",\"ienne\")\n",
    "                elif  entity.text.endswith(\"if\") or entity.text.endswith(\"ive\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"if\",\"ive\")\n",
    "                elif  entity.text.endswith(\"er\") or entity.text.endswith(\"ère\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"er\",\"ère\")\n",
    "                elif  entity.text.endswith(\"ier\") or entity.text.endswith(\"ière\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"ier\",\"ière\")\n",
    "                elif  entity.text.endswith(\"on\") or entity.text.endswith(\"onne\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"on\",\"ne\")\n",
    "                elif  entity.text.endswith(\"eur\") or entity.text.endswith(\"euse\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"eur\",\"euse\")\n",
    "                elif  entity.text.endswith(\"leur\") or entity.text.endswith(\"leure\"): #meilleur(e)\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"eur\",\"eure\")\n",
    "                elif entity.text.endswith(\"é\") or entity.text.endswith(\"ée\"):\n",
    "                    strr= strr+add_inclusive(entity.text,\"ée\")+\" \"\n",
    "                else :\n",
    "                    strr= strr+entity.text+\" \"\n",
    "        \n",
    "        \n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'NOUN'  :                       \n",
    "                if entity.text.endswith(\"teur\") or entity.text.endswith(\"trice\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"teur\",\"trice\")\n",
    "                elif entity.text.endswith(\"ieur\") or entity.text.endswith(\"ieure\"):\n",
    "                    strr= strr+\" \"+add_inclusive_adj(entity.text,\"eur\",\"eure\")\n",
    "                elif entity.text.endswith(\"peur\") or entity.text.endswith(\"peuse\"): #développeur(euse)\n",
    "                    strr= strr+add_inclusive_adj(entity.text,\"eur\",\"euse\")+\" \"\n",
    "                else :\n",
    "                    strr= strr+\" \"+entity.text+\" \"\n",
    "                                    \n",
    "                    \n",
    "                \n",
    "            elif  entity.text == 'la' or entity.text == 'le' or entity.text == 'La' or entity.text == 'Le'  :                    \n",
    "                strr= strr+\"le/la\"+\" \"\n",
    "                \n",
    "            elif  entity.text == 'il' or entity.text == 'elle' or entity.text == 'Il' or entity.text == 'Elle'   :                    \n",
    "                strr= strr+\"il/elle\"   +\" \"             \n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'ADP'  :\n",
    "                strr= strr+\" \"+entity.text+\" \"\n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'AUX'  :\n",
    "                strr= strr+\" \"+entity.text+\" \"\n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'DET'  :\n",
    "                strr= strr+entity.text  +\" \"  \n",
    "            elif  re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'CCONJ'  :\n",
    "                strr= strr+\" \" +entity.text+\" \"    \n",
    "            elif  entity.text == '.' or entity.text == ','  :\n",
    "                strr= strr+\" \" +entity.text+\" \"    \n",
    "                                                       \n",
    "                               \n",
    "            else :\n",
    "                 strr= strr+\"\"+entity.text\n",
    "    return strr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entitiesListName = []\n",
    "def merging_entities(sentence):\n",
    "    entitiesList = []\n",
    "\n",
    "    \n",
    "    for entity in sentence.get_spans('fr-ner')  :\n",
    "        \n",
    "        for data in entity.labels:\n",
    "            if re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'PER' and len(entity.text)>2 :\n",
    "                #strr= strr+\"[REDACTEDPER]\"\n",
    "                entitiesList.append(entity.text)\n",
    "                entitiesListName.append('PER')\n",
    "                \n",
    "            if re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'LOC ' and len(entity.text)>2 :\n",
    "                #strr= strr+\"[REDACTEDLOC]\"\n",
    "                entitiesList.append(entity.text)\n",
    "                entitiesListName.append('LOC')\n",
    "                \n",
    "            if re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'MISC' and len(entity.text)>2 :\n",
    "                #strr= strr+\"[REDACTEDMISC]\"\n",
    "                entitiesList.append(entity.text)\n",
    "                entitiesListName.append('MISC')\n",
    "                \n",
    "                                                \n",
    "            if re.sub(r'\\([^)]*\\)', '', str(data))[:-1] == 'ORG' and len(entity.text)>2 :\n",
    "               # strr= strr+\"[REDACTEDORG]\"\n",
    "                entitiesList.append(entity.text)\n",
    "                entitiesListName.append('ORG')\n",
    "                \n",
    "        \n",
    "    return entitiesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajout de nouvelles entités grâce au rule-based entities de Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_email(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EMAIL\")\n",
    "    doc.ents += (entity,)\n",
    "    \n",
    "def add_url(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"URL\")\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "def add_tel(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"TEL\")\n",
    "    doc.ents += (entity,)\n",
    "    \n",
    "def add_Sdate(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"S-DATE\")\n",
    "    doc.ents += (entity,)\n",
    "def add_date(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"DATE\")\n",
    "    doc.ents += (entity,)\n",
    "        \n",
    "def add_verb(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"VERB_FEM\")\n",
    "    doc.ents += (entity,)\n",
    "def add_adj(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"adj fem\")\n",
    "    doc.ents += (entity,)\n",
    "def add_ssNum(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"NUM_SEC_SO\")\n",
    "    doc.ents += (entity,)\n",
    "def add_age(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"AGE\")\n",
    "    doc.ents += (entity,)\n",
    "def add_situation_fam(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"SIT_FAM\")\n",
    "    doc.ents += (entity,)\n",
    "def add_formation(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"FORMATION\")\n",
    "    doc.ents += (entity,)\n",
    "def add_enfants(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"nbr_enfants\")\n",
    "    doc.ents += (entity,)\n",
    "def add_sexe(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"sexe\")\n",
    "    doc.ents += (entity,)\n",
    "\n",
    "def add_verb(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"verb\")\n",
    "    doc.ents += (entity,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternMail = [{\"LIKE_EMAIL\": True}]\n",
    "patternUrl = [{\"LIKE_URL\": True}]\n",
    "patternTel = [{\"TEXT\": {\"REGEX\": \"^\\+*\\d{11}$\"}}]\n",
    "patternSecS = [{\"TEXT\": {\"REGEX\": \"^\\+*\\d{13}$\"}}]\n",
    "#[{\"SHAPE\": \"d\"}, {\"SHAPE\": \"dd\"}, {\"SHAPE\": \"dd\"}, {\"SHAPE\": \"dd\"}, {\"SHAPE\": \"ddd\"}, {\"SHAPE\": \"ddd\"}]\n",
    "patternSDate = [{\"SHAPE\": \"dddd\", \"LENGTH\": 4}]\n",
    "patternDate = [{\"TEXT\": {\"REGEX\": \"^(0[1-9]|[12][0-9]|3[01])[- /.](0[1-9]|1[012])[- /.](19|20)\\d\\d$\"}}]\n",
    "patternVerbFem = [{\"POS\": \"VERB\",\"TEXT\": {\"REGEX\": \"ée$\"}}]\n",
    "patternAge = [{\"ORTH\": \"age\"}, {\"ORTH\": \":\"}, {}]\n",
    "patternAge2 = [{\"ORTH\": \"a\"},{},{\"ORTH\": \"ans\"} ]\n",
    "patternFormation = [{\"ORTH\": \"de\"},{},{\"ORTH\": \"ans\"} ]\n",
    "patternEnfants = [{},{\"ORTH\": \"enfants\"} ,{\"ORTH\": \"s\", \"OP\":\"?\"}]\n",
    "patternFamil =[{\"TEXT\": {\"REGEX\": \"^marié(e)$\"}}]\n",
    "               # {\"ORTH\": \"pacsé(e)\", \"OP\":\"?\"},{\"ORTH\": \"divorcé(e)\", \"OP\":\"?\"},{\"ORTH\": \"séparé(e)\", \"OP\":\"?\"},{\"ORTH\": \"célibataire\", \"OP\":\"?\"},{\"ORTH\": \"veuf\", \"OP\":\"?\"} ]\n",
    "patternSexe = [{\"ORTH\": \"(M)\", \"OP\":\"?\"},{\"ORTH\": \"(F)\", \"OP\":\"?\"} ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"mail\", add_email, patternMail)\n",
    "matcher.add(\"url\", add_url, patternUrl)\n",
    "matcher.add(\"tel\", add_tel, patternTel)\n",
    "matcher.add(\"sec\", add_ssNum, patternSecS)\n",
    "matcher.add(\"date\", add_date, patternDate)\n",
    "matcher.add(\"Sdate\", add_Sdate, patternSDate)\n",
    "#matcher.add(\"VerbFem\", add_verb, patternVerbFem)\n",
    "matcher.add(\"Age\", add_age, patternAge)\n",
    "matcher.add(\"Age2\", add_age, patternAge2)\n",
    "matcher.add(\"situation\", add_situation_fam, patternFamil)\n",
    "matcher.add(\"formation\", add_formation, patternFormation)\n",
    "matcher.add(\"enfants\", add_enfants, patternEnfants)\n",
    "matcher.add(\"sexe\", add_sexe, patternSexe)\n",
    "#matcher.add(\"verb\", add_verb, patternVerb)\n",
    "#matcher.add(\"adj\", add_adj, pattern1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "df_right['Anonym']=spacy.tokens.doc.Doc\n",
    "\n",
    "for i in df_right['value']:\n",
    "    doc = nlp(i)\n",
    "    df_right['Anonym'][count]=doc\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_entities(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'PER':\n",
    "        return '[REDACTEDPER] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'MISC':\n",
    "        return '[REDACTEDMISC] '    \n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'LOC':\n",
    "        return '[REDACTEDLOC] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'EMAIL':\n",
    "        return '[REDACTEDEMAIL] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'TEL':\n",
    "        return '[REDACTEDTEL] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'URL':\n",
    "        return '[REDACTEDURL] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'S-DATE':\n",
    "        return '[REDACTEDS-DATE] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'DATE':\n",
    "        return '[REDACTEDDATE] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'NUM_SEC_SO':\n",
    "        return '[REDACTEDNUM_SEC_SO] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'AGE':\n",
    "        return '[REDACTEDAGE] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'SIT_FAM':\n",
    "        return '[REDACTEDSIT_FAM] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'FORMATION':\n",
    "        return '[REDACTEDFORMATION] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'NBR_ENFANTS':\n",
    "        return '[REDACTEDNBR_ENFANTS] '\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'VERB_FEM':\n",
    "        return token.text[:-1]+' '    \n",
    "    if token.text == 'elle':\n",
    "        return 'il '    \n",
    "    return token.string\n",
    "\n",
    "def redact_names(nlp_doc):\n",
    "    for ent in nlp_doc.ents:        \n",
    "        ent.merge()\n",
    "    tokens = map(anonymize_entities, nlp_doc)\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize(phrase):\n",
    "    sentence = Sentence(phrase)\n",
    "    # predict PoS tags\n",
    "    tagger.predict(sentence)\n",
    "  \n",
    "\n",
    "    i=0\n",
    "    for ent in merging_entities(sentence) :\n",
    "        if ent in phrase:\n",
    "            phrase=phrase.replace(ent,entitiesListName[i])\n",
    "        i=i+1    \n",
    "     \n",
    "    \n",
    "    sentence2 = Sentence(phrase)\n",
    "    # predict PoS tags\n",
    "    tagger.predict(sentence2)\n",
    "    \n",
    "  \n",
    "    doc = nlp(merging_inclusive(sentence2))\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return redact_names(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_right['Anonym']:\n",
    "    print('Before Anonymisation: ',i)\n",
    "    print(anonymize(str(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}