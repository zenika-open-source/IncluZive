{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "import flair\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
    "from typing import List\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus  = flair.datasets.WIKINER_FRENCH().downsample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    TransformerWordEmbeddings('camembert-base')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize trainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:06:14,027 Reading data from /Users/amalbedoui/.flair/datasets/wikiner_french\n",
      "2020-12-06 14:06:14,027 Train: /Users/amalbedoui/.flair/datasets/wikiner_french/aij-wikiner-fr-wp3.train\n",
      "2020-12-06 14:06:14,028 Dev: None\n",
      "2020-12-06 14:06:14,029 Test: None\n",
      "2020-12-06 14:06:26,464 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,466 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 14:06:26,468 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,469 Corpus: \"Corpus: 10713 train + 1190 dev + 1323 test sentences\"\n",
      "2020-12-06 14:06:26,470 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,470 Parameters:\n",
      "2020-12-06 14:06:26,471  - learning_rate: \"0.1\"\n",
      "2020-12-06 14:06:26,472  - mini_batch_size: \"32\"\n",
      "2020-12-06 14:06:26,473  - patience: \"3\"\n",
      "2020-12-06 14:06:26,474  - anneal_factor: \"0.5\"\n",
      "2020-12-06 14:06:26,474  - max_epochs: \"3\"\n",
      "2020-12-06 14:06:26,475  - shuffle: \"True\"\n",
      "2020-12-06 14:06:26,476  - train_with_dev: \"False\"\n",
      "2020-12-06 14:06:26,477  - batch_growth_annealing: \"False\"\n",
      "2020-12-06 14:06:26,478 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,478 Model training base path: \"resources/taggers/example-ner-camembert\"\n",
      "2020-12-06 14:06:26,479 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,480 Device: cpu\n",
      "2020-12-06 14:06:26,480 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:06:26,481 Embeddings storage mode: cpu\n",
      "2020-12-06 14:06:26,485 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:09:52,514 epoch 1 - iter 33/335 - loss 18.85467373 - samples/sec: 5.15 - lr: 0.100000\n",
      "2020-12-06 14:14:12,173 epoch 1 - iter 66/335 - loss 13.96771589 - samples/sec: 4.08 - lr: 0.100000\n",
      "2020-12-06 14:18:26,668 epoch 1 - iter 99/335 - loss 11.49353444 - samples/sec: 4.16 - lr: 0.100000\n",
      "2020-12-06 14:22:45,629 epoch 1 - iter 132/335 - loss 9.86136016 - samples/sec: 4.09 - lr: 0.100000\n",
      "2020-12-06 14:27:03,565 epoch 1 - iter 165/335 - loss 8.71029306 - samples/sec: 4.11 - lr: 0.100000\n",
      "2020-12-06 14:31:22,650 epoch 1 - iter 198/335 - loss 7.90817870 - samples/sec: 4.09 - lr: 0.100000\n",
      "2020-12-06 14:35:39,097 epoch 1 - iter 231/335 - loss 7.26732744 - samples/sec: 4.13 - lr: 0.100000\n",
      "2020-12-06 14:39:56,775 epoch 1 - iter 264/335 - loss 6.76568408 - samples/sec: 4.11 - lr: 0.100000\n",
      "2020-12-06 14:44:17,696 epoch 1 - iter 297/335 - loss 6.37636328 - samples/sec: 4.06 - lr: 0.100000\n",
      "2020-12-06 14:48:36,069 epoch 1 - iter 330/335 - loss 6.04369914 - samples/sec: 4.10 - lr: 0.100000\n",
      "2020-12-06 14:49:17,010 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:49:17,012 EPOCH 1 done: loss 5.9993 - lr 0.1000000\n",
      "2020-12-06 14:52:54,286 DEV : loss 2.063162088394165 - score 0.8262\n",
      "2020-12-06 14:52:55,716 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 14:52:57,086 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 14:57:20,621 epoch 2 - iter 33/335 - loss 2.72406739 - samples/sec: 4.03 - lr: 0.100000\n",
      "2020-12-06 15:01:41,987 epoch 2 - iter 66/335 - loss 2.91239265 - samples/sec: 4.05 - lr: 0.100000\n",
      "2020-12-06 15:05:35,165 epoch 2 - iter 99/335 - loss 2.86376648 - samples/sec: 4.54 - lr: 0.100000\n",
      "2020-12-06 15:09:30,297 epoch 2 - iter 132/335 - loss 2.81526413 - samples/sec: 4.51 - lr: 0.100000\n",
      "2020-12-06 15:13:43,881 epoch 2 - iter 165/335 - loss 2.72648113 - samples/sec: 4.18 - lr: 0.100000\n",
      "2020-12-06 15:18:01,276 epoch 2 - iter 198/335 - loss 2.66204009 - samples/sec: 4.11 - lr: 0.100000\n",
      "2020-12-06 15:22:24,657 epoch 2 - iter 231/335 - loss 2.60553318 - samples/sec: 4.02 - lr: 0.100000\n",
      "2020-12-06 15:26:41,717 epoch 2 - iter 264/335 - loss 2.56050891 - samples/sec: 4.12 - lr: 0.100000\n",
      "2020-12-06 15:31:06,765 epoch 2 - iter 297/335 - loss 2.51484690 - samples/sec: 4.00 - lr: 0.100000\n",
      "2020-12-06 15:35:26,427 epoch 2 - iter 330/335 - loss 2.48795096 - samples/sec: 4.08 - lr: 0.100000\n",
      "2020-12-06 15:36:02,209 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 15:36:02,210 EPOCH 2 done: loss 2.4844 - lr 0.1000000\n",
      "2020-12-06 15:38:52,172 DEV : loss 1.7164278030395508 - score 0.8372\n",
      "2020-12-06 15:38:53,207 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 15:38:54,106 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 15:42:07,134 epoch 3 - iter 33/335 - loss 1.99655330 - samples/sec: 5.49 - lr: 0.100000\n",
      "2020-12-06 15:45:22,613 epoch 3 - iter 66/335 - loss 2.02523709 - samples/sec: 5.42 - lr: 0.100000\n",
      "2020-12-06 15:48:36,947 epoch 3 - iter 99/335 - loss 2.00852589 - samples/sec: 5.45 - lr: 0.100000\n",
      "2020-12-06 15:52:33,212 epoch 3 - iter 132/335 - loss 1.98886125 - samples/sec: 4.48 - lr: 0.100000\n",
      "2020-12-06 15:57:51,586 epoch 3 - iter 165/335 - loss 2.01746630 - samples/sec: 3.33 - lr: 0.100000\n",
      "2020-12-06 16:04:03,336 epoch 3 - iter 198/335 - loss 2.00466948 - samples/sec: 2.85 - lr: 0.100000\n",
      "2020-12-06 16:08:17,392 epoch 3 - iter 231/335 - loss 1.98160487 - samples/sec: 4.17 - lr: 0.100000\n",
      "2020-12-06 16:11:37,912 epoch 3 - iter 264/335 - loss 1.96729905 - samples/sec: 5.28 - lr: 0.100000\n",
      "2020-12-06 16:15:09,023 epoch 3 - iter 297/335 - loss 1.97783431 - samples/sec: 5.02 - lr: 0.100000\n",
      "2020-12-06 16:18:46,663 epoch 3 - iter 330/335 - loss 1.95914565 - samples/sec: 4.87 - lr: 0.100000\n",
      "2020-12-06 16:19:18,380 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 16:19:18,381 EPOCH 3 done: loss 1.9552 - lr 0.1000000\n",
      "2020-12-06 16:22:24,288 DEV : loss 1.3991682529449463 - score 0.855\n",
      "2020-12-06 16:22:25,524 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 16:22:27,047 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 16:22:27,048 Testing using best model ...\n",
      "2020-12-06 16:22:27,049 loading file resources/taggers/example-ner-camembert/best-model.pt\n",
      "2020-12-06 16:25:53,100 0.8351\t0.8351\t0.8351\n",
      "2020-12-06 16:25:53,104 \n",
      "Results:\n",
      "- F1-score (micro) 0.8351\n",
      "- F1-score (macro) 0.8112\n",
      "\n",
      "By class:\n",
      "LOC        tp: 992 - fp: 234 - fn: 176 - precision: 0.8091 - recall: 0.8493 - f1-score: 0.8287\n",
      "MISC       tp: 264 - fp: 96 - fn: 119 - precision: 0.7333 - recall: 0.6893 - f1-score: 0.7106\n",
      "ORG        tp: 178 - fp: 24 - fn: 80 - precision: 0.8812 - recall: 0.6899 - f1-score: 0.7739\n",
      "PER        tp: 658 - fp: 59 - fn: 38 - precision: 0.9177 - recall: 0.9454 - f1-score: 0.9314\n",
      "2020-12-06 16:25:53,105 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8351297405189619,\n",
       " 'dev_score_history': [0.8262095899327403,\n",
       "  0.8371689101172383,\n",
       "  0.8550347222222222],\n",
       " 'train_loss_history': [5.999284717929897,\n",
       "  2.484362497614391,\n",
       "  1.955211645809572],\n",
       " 'dev_loss_history': [2.063162088394165,\n",
       "  1.7164278030395508,\n",
       "  1.3991682529449463]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner-camembert',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=3,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-08 13:43:06,222 loading file resources/taggers/example-ner-camembert/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "model = SequenceTagger.load('resources/taggers/example-ner-camembert/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example sentence\n",
    "sentence = Sentence('Emma Louise, habite au 26 rue Alexandre, 75005 Paris, France, née le 11/11/1993. Elle travaille chez Zenika et elle est joignable sur 06660006.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 9.91 ms, total: 182 ms\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict tags and print\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma <B-PER> Louise <E-PER> , habite au 26 rue <B-LOC> Alexandre <E-LOC> , 75005 <B-LOC> Paris <E-LOC> , France <S-LOC> , née le 11 / 11 / 1993 . Elle travaille chez Zenika <S-ORG> et elle est joignable sur 06660006 .\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
