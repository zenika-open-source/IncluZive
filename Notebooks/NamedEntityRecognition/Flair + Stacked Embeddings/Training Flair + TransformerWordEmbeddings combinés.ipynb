{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "import flair\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
    "from typing import List\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus  = flair.datasets.WIKINER_FRENCH().downsample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    TransformerWordEmbeddings('camembert-base'),\n",
    "    TransformerWordEmbeddings('illuin/lepetit'), \n",
    "    TransformerWordEmbeddings('flaubert/flaubert_large_cased')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize trainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 22:42:05,501 Reading data from /Users/amalbedoui/.flair/datasets/wikiner_french\n",
      "2020-12-06 22:42:05,501 Train: /Users/amalbedoui/.flair/datasets/wikiner_french/aij-wikiner-fr-wp3.train\n",
      "2020-12-06 22:42:05,501 Dev: None\n",
      "2020-12-06 22:42:05,502 Test: None\n",
      "2020-12-06 22:42:32,627 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,636 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 256, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 256, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 256)\n",
      "          (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): TransformerWordEmbeddings(\n",
      "      (model): FlaubertModel(\n",
      "        (position_embeddings): Embedding(512, 1024)\n",
      "        (embeddings): Embedding(68729, 1024, padding_idx=2)\n",
      "        (layer_norm_emb): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attentions): ModuleList(\n",
      "          (0): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (1): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (2): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (3): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (4): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (5): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (6): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (7): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (8): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (9): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (10): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (11): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (12): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (13): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (14): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (15): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (16): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (17): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (18): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (19): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (20): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (21): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (22): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (23): MultiHeadAttention(\n",
      "            (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (layer_norm1): ModuleList(\n",
      "          (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (4): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (5): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (6): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (7): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (8): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (9): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (10): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (11): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (12): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (13): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (14): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (15): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (16): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (17): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (18): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (19): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (20): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (21): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (22): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (23): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "        (ffns): ModuleList(\n",
      "          (0): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (1): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (2): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (3): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (4): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (5): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (6): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (7): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (8): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (9): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (10): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (11): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (12): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (13): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (14): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (15): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (16): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (17): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (18): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (19): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (20): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (21): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (22): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "          (23): TransformerFFN(\n",
      "            (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (layer_norm2): ModuleList(\n",
      "          (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (3): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (4): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (5): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (6): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (7): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (8): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (9): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (10): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (11): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (12): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (13): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (14): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (15): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (16): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (17): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (18): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (19): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (20): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (21): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (22): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (23): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "  (rnn): LSTM(8192, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 22:42:32,639 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,640 Corpus: \"Corpus: 10713 train + 1190 dev + 1323 test sentences\"\n",
      "2020-12-06 22:42:32,642 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,643 Parameters:\n",
      "2020-12-06 22:42:32,644  - learning_rate: \"0.1\"\n",
      "2020-12-06 22:42:32,644  - mini_batch_size: \"32\"\n",
      "2020-12-06 22:42:32,645  - patience: \"3\"\n",
      "2020-12-06 22:42:32,646  - anneal_factor: \"0.5\"\n",
      "2020-12-06 22:42:32,647  - max_epochs: \"3\"\n",
      "2020-12-06 22:42:32,648  - shuffle: \"True\"\n",
      "2020-12-06 22:42:32,649  - train_with_dev: \"False\"\n",
      "2020-12-06 22:42:32,649  - batch_growth_annealing: \"False\"\n",
      "2020-12-06 22:42:32,650 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,651 Model training base path: \"resources/taggers/example-ner-combined\"\n",
      "2020-12-06 22:42:32,652 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,653 Device: cpu\n",
      "2020-12-06 22:42:32,654 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:42:32,655 Embeddings storage mode: cpu\n",
      "2020-12-06 22:42:32,666 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:56:47,765 epoch 1 - iter 33/335 - loss 24.89682255 - samples/sec: 1.24 - lr: 0.100000\n",
      "2020-12-06 23:10:53,578 epoch 1 - iter 66/335 - loss 21.85902632 - samples/sec: 1.25 - lr: 0.100000\n",
      "2020-12-06 23:26:08,732 epoch 1 - iter 99/335 - loss 19.57645811 - samples/sec: 1.15 - lr: 0.100000\n",
      "2020-12-06 23:42:10,682 epoch 1 - iter 132/335 - loss 17.97804561 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-06 23:58:12,703 epoch 1 - iter 165/335 - loss 16.64100697 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 00:14:09,148 epoch 1 - iter 198/335 - loss 15.58203350 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 00:29:38,520 epoch 1 - iter 231/335 - loss 14.68618817 - samples/sec: 1.14 - lr: 0.100000\n",
      "2020-12-07 00:46:02,039 epoch 1 - iter 264/335 - loss 13.93580489 - samples/sec: 1.07 - lr: 0.100000\n",
      "2020-12-07 01:02:01,354 epoch 1 - iter 297/335 - loss 13.26891663 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 01:17:19,097 epoch 1 - iter 330/335 - loss 12.71303656 - samples/sec: 1.15 - lr: 0.100000\n",
      "2020-12-07 01:19:36,530 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 01:19:36,531 EPOCH 1 done: loss 12.6473 - lr 0.1000000\n",
      "2020-12-07 01:31:53,118 DEV : loss 6.80029296875 - score 0.6179\n",
      "2020-12-07 01:31:54,415 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 01:31:59,803 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 01:49:29,310 epoch 2 - iter 33/335 - loss 7.24281282 - samples/sec: 1.01 - lr: 0.100000\n",
      "2020-12-07 02:05:30,176 epoch 2 - iter 66/335 - loss 7.13519774 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 02:21:57,328 epoch 2 - iter 99/335 - loss 7.13617112 - samples/sec: 1.07 - lr: 0.100000\n",
      "2020-12-07 02:37:16,851 epoch 2 - iter 132/335 - loss 7.01209945 - samples/sec: 1.15 - lr: 0.100000\n",
      "2020-12-07 02:52:38,726 epoch 2 - iter 165/335 - loss 6.90020599 - samples/sec: 1.15 - lr: 0.100000\n",
      "2020-12-07 03:08:08,525 epoch 2 - iter 198/335 - loss 6.84756147 - samples/sec: 1.14 - lr: 0.100000\n",
      "2020-12-07 03:23:39,975 epoch 2 - iter 231/335 - loss 6.67689033 - samples/sec: 1.13 - lr: 0.100000\n",
      "2020-12-07 03:39:13,873 epoch 2 - iter 264/335 - loss 6.49618798 - samples/sec: 1.13 - lr: 0.100000\n",
      "2020-12-07 03:55:30,929 epoch 2 - iter 297/335 - loss 6.45514044 - samples/sec: 1.08 - lr: 0.100000\n",
      "2020-12-07 04:10:50,486 epoch 2 - iter 330/335 - loss 6.37075330 - samples/sec: 1.15 - lr: 0.100000\n",
      "2020-12-07 04:13:04,057 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 04:13:04,058 EPOCH 2 done: loss 6.3616 - lr 0.1000000\n",
      "2020-12-07 04:24:46,337 DEV : loss 4.265444278717041 - score 0.7026\n",
      "2020-12-07 04:24:47,344 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 04:24:51,943 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 04:41:03,918 epoch 3 - iter 33/335 - loss 5.72448241 - samples/sec: 1.09 - lr: 0.100000\n",
      "2020-12-07 04:56:53,037 epoch 3 - iter 66/335 - loss 5.43655995 - samples/sec: 1.11 - lr: 0.100000\n",
      "2020-12-07 05:12:51,488 epoch 3 - iter 99/335 - loss 5.27326622 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 05:28:22,657 epoch 3 - iter 132/335 - loss 5.12110393 - samples/sec: 1.13 - lr: 0.100000\n",
      "2020-12-07 05:44:08,808 epoch 3 - iter 165/335 - loss 5.05767758 - samples/sec: 1.12 - lr: 0.100000\n",
      "2020-12-07 06:00:17,995 epoch 3 - iter 198/335 - loss 4.95977659 - samples/sec: 1.09 - lr: 0.100000\n",
      "2020-12-07 06:16:22,959 epoch 3 - iter 231/335 - loss 4.87270974 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 06:32:36,428 epoch 3 - iter 264/335 - loss 4.81818462 - samples/sec: 1.09 - lr: 0.100000\n",
      "2020-12-07 06:48:42,269 epoch 3 - iter 297/335 - loss 4.75323534 - samples/sec: 1.09 - lr: 0.100000\n",
      "2020-12-07 07:04:44,351 epoch 3 - iter 330/335 - loss 4.68365730 - samples/sec: 1.10 - lr: 0.100000\n",
      "2020-12-07 07:07:04,122 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 07:07:04,123 EPOCH 3 done: loss 4.6750 - lr 0.1000000\n",
      "2020-12-07 07:18:49,959 DEV : loss 3.039752244949341 - score 0.7683\n",
      "2020-12-07 07:18:50,965 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 07:18:57,828 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 07:18:57,831 Testing using best model ...\n",
      "2020-12-07 07:18:57,832 loading file resources/taggers/example-ner-combined/best-model.pt\n",
      "2020-12-07 07:32:16,960 0.7542\t0.7468\t0.7504\n",
      "2020-12-07 07:32:16,962 \n",
      "Results:\n",
      "- F1-score (micro) 0.7504\n",
      "- F1-score (macro) 0.6806\n",
      "\n",
      "By class:\n",
      "LOC        tp: 911 - fp: 283 - fn: 223 - precision: 0.7630 - recall: 0.8034 - f1-score: 0.7826\n",
      "MISC       tp: 217 - fp: 141 - fn: 208 - precision: 0.6061 - recall: 0.5106 - f1-score: 0.5543\n",
      "ORG        tp: 123 - fp: 104 - fn: 151 - precision: 0.5419 - recall: 0.4489 - f1-score: 0.4910\n",
      "PER        tp: 648 - fp: 91 - fn: 62 - precision: 0.8769 - recall: 0.9127 - f1-score: 0.8944\n",
      "2020-12-07 07:32:16,963 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.7504445761707172,\n",
       " 'dev_score_history': [0.6179429275302813,\n",
       "  0.7025683512841755,\n",
       "  0.7682775712515489],\n",
       " 'train_loss_history': [12.647268992751393,\n",
       "  6.36160932939444,\n",
       "  4.675048357693117],\n",
       " 'dev_loss_history': [6.80029296875, 4.265444278717041, 3.039752244949341]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner-combined',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=3,\n",
    "              checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceTagger.load('resources/taggers/example-ner-combined/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example sentence\n",
    "sentence = Sentence('Emma Louise, habite au 26 rue Alexandre, 75005 Paris, France, née le 11/11/1993. Elle travaille chez Zenika et elle est joignable sur 06660006.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 3.1 s, total: 4.14 s\n",
      "Wall time: 8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict tags and print\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma <B-PER> Louise <E-PER> , habite au 26 rue Alexandre <E-PER> , 75005 <B-LOC> Paris <E-LOC> , France <S-LOC> , née le 11 / 11 / 1993 . Elle travaille chez Zenika <S-MISC> et elle est joignable sur 06660006 .\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
