{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "import flair\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
    "from typing import List\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus  = flair.datasets.WIKINER_FRENCH().downsample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    TransformerWordEmbeddings('illuin/lepetit')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 22:05:19,155 Reading data from /Users/amalbedoui/.flair/datasets/wikiner_french\n",
      "2020-12-06 22:05:19,156 Train: /Users/amalbedoui/.flair/datasets/wikiner_french/aij-wikiner-fr-wp3.train\n",
      "2020-12-06 22:05:19,156 Dev: None\n",
      "2020-12-06 22:05:19,156 Test: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fad103e61444b59869012d71f5cb271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=507.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8daa4e15cb714c6fb6ebda3671df169e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=810912.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866227e18fd042ffa32b7c93397c7579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fc96e96edc45e1b109301df65f9b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd8d14646324896b30480b5d2e03b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=71917496.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-06 22:05:39,983 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,985 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 256, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 256, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 256)\n",
      "          (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (rnn): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-06 22:05:39,986 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,987 Corpus: \"Corpus: 10713 train + 1190 dev + 1323 test sentences\"\n",
      "2020-12-06 22:05:39,988 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,988 Parameters:\n",
      "2020-12-06 22:05:39,989  - learning_rate: \"0.1\"\n",
      "2020-12-06 22:05:39,989  - mini_batch_size: \"32\"\n",
      "2020-12-06 22:05:39,990  - patience: \"3\"\n",
      "2020-12-06 22:05:39,991  - anneal_factor: \"0.5\"\n",
      "2020-12-06 22:05:39,991  - max_epochs: \"3\"\n",
      "2020-12-06 22:05:39,992  - shuffle: \"True\"\n",
      "2020-12-06 22:05:39,993  - train_with_dev: \"False\"\n",
      "2020-12-06 22:05:39,993  - batch_growth_annealing: \"False\"\n",
      "2020-12-06 22:05:39,994 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,995 Model training base path: \"resources/taggers/example-ner-lepetit\"\n",
      "2020-12-06 22:05:39,996 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,996 Device: cpu\n",
      "2020-12-06 22:05:39,997 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:05:39,998 Embeddings storage mode: cpu\n",
      "2020-12-06 22:05:40,001 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:06:36,934 epoch 1 - iter 33/335 - loss 18.20141749 - samples/sec: 18.77 - lr: 0.100000\n",
      "2020-12-06 22:07:29,548 epoch 1 - iter 66/335 - loss 13.62345836 - samples/sec: 20.25 - lr: 0.100000\n",
      "2020-12-06 22:08:24,522 epoch 1 - iter 99/335 - loss 11.41923634 - samples/sec: 19.38 - lr: 0.100000\n",
      "2020-12-06 22:09:18,330 epoch 1 - iter 132/335 - loss 10.06091954 - samples/sec: 19.83 - lr: 0.100000\n",
      "2020-12-06 22:10:13,344 epoch 1 - iter 165/335 - loss 9.19170836 - samples/sec: 19.36 - lr: 0.100000\n",
      "2020-12-06 22:11:07,460 epoch 1 - iter 198/335 - loss 8.48982354 - samples/sec: 19.65 - lr: 0.100000\n",
      "2020-12-06 22:12:00,919 epoch 1 - iter 231/335 - loss 7.92765089 - samples/sec: 19.96 - lr: 0.100000\n",
      "2020-12-06 22:12:56,616 epoch 1 - iter 264/335 - loss 7.48148157 - samples/sec: 19.12 - lr: 0.100000\n",
      "2020-12-06 22:13:50,549 epoch 1 - iter 297/335 - loss 7.08880717 - samples/sec: 19.79 - lr: 0.100000\n",
      "2020-12-06 22:14:46,364 epoch 1 - iter 330/335 - loss 6.75817940 - samples/sec: 19.09 - lr: 0.100000\n",
      "2020-12-06 22:14:54,259 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:14:54,260 EPOCH 1 done: loss 6.7157 - lr 0.1000000\n",
      "2020-12-06 22:15:30,594 DEV : loss 2.9247255325317383 - score 0.735\n",
      "2020-12-06 22:15:31,478 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 22:15:31,654 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:16:25,176 epoch 2 - iter 33/335 - loss 3.83095719 - samples/sec: 19.95 - lr: 0.100000\n",
      "2020-12-06 22:17:19,136 epoch 2 - iter 66/335 - loss 3.69902801 - samples/sec: 19.75 - lr: 0.100000\n",
      "2020-12-06 22:18:13,080 epoch 2 - iter 99/335 - loss 3.60191875 - samples/sec: 19.72 - lr: 0.100000\n",
      "2020-12-06 22:19:06,921 epoch 2 - iter 132/335 - loss 3.51179946 - samples/sec: 19.82 - lr: 0.100000\n",
      "2020-12-06 22:20:01,414 epoch 2 - iter 165/335 - loss 3.55266288 - samples/sec: 19.55 - lr: 0.100000\n",
      "2020-12-06 22:20:57,909 epoch 2 - iter 198/335 - loss 3.49500127 - samples/sec: 18.88 - lr: 0.100000\n",
      "2020-12-06 22:21:52,942 epoch 2 - iter 231/335 - loss 3.47486816 - samples/sec: 19.33 - lr: 0.100000\n",
      "2020-12-06 22:22:48,901 epoch 2 - iter 264/335 - loss 3.42884256 - samples/sec: 19.06 - lr: 0.100000\n",
      "2020-12-06 22:23:43,978 epoch 2 - iter 297/335 - loss 3.39116494 - samples/sec: 19.34 - lr: 0.100000\n",
      "2020-12-06 22:24:40,327 epoch 2 - iter 330/335 - loss 3.35817738 - samples/sec: 18.93 - lr: 0.100000\n",
      "2020-12-06 22:24:48,521 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:24:48,522 EPOCH 2 done: loss 3.3600 - lr 0.1000000\n",
      "2020-12-06 22:25:29,868 DEV : loss 2.4928743839263916 - score 0.7415\n",
      "2020-12-06 22:25:30,892 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 22:25:31,083 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:26:31,630 epoch 3 - iter 33/335 - loss 2.93943920 - samples/sec: 17.66 - lr: 0.100000\n",
      "2020-12-06 22:27:29,994 epoch 3 - iter 66/335 - loss 2.89474054 - samples/sec: 18.25 - lr: 0.100000\n",
      "2020-12-06 22:28:31,041 epoch 3 - iter 99/335 - loss 2.86238155 - samples/sec: 17.47 - lr: 0.100000\n",
      "2020-12-06 22:29:28,729 epoch 3 - iter 132/335 - loss 2.88973037 - samples/sec: 18.47 - lr: 0.100000\n",
      "2020-12-06 22:30:26,332 epoch 3 - iter 165/335 - loss 2.85489126 - samples/sec: 18.52 - lr: 0.100000\n",
      "2020-12-06 22:31:21,780 epoch 3 - iter 198/335 - loss 2.81596945 - samples/sec: 19.23 - lr: 0.100000\n",
      "2020-12-06 22:32:19,745 epoch 3 - iter 231/335 - loss 2.81527900 - samples/sec: 18.38 - lr: 0.100000\n",
      "2020-12-06 22:33:15,431 epoch 3 - iter 264/335 - loss 2.79831878 - samples/sec: 19.16 - lr: 0.100000\n",
      "2020-12-06 22:34:11,050 epoch 3 - iter 297/335 - loss 2.75427440 - samples/sec: 19.16 - lr: 0.100000\n",
      "2020-12-06 22:35:07,579 epoch 3 - iter 330/335 - loss 2.72130010 - samples/sec: 18.84 - lr: 0.100000\n",
      "2020-12-06 22:35:15,252 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:35:15,252 EPOCH 3 done: loss 2.7185 - lr 0.1000000\n",
      "2020-12-06 22:35:52,523 DEV : loss 2.0861685276031494 - score 0.7702\n",
      "2020-12-06 22:35:53,489 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-06 22:35:53,759 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-06 22:35:53,760 Testing using best model ...\n",
      "2020-12-06 22:35:53,762 loading file resources/taggers/example-ner-lepetit/best-model.pt\n",
      "2020-12-06 22:36:36,568 0.7626\t0.7531\t0.7578\n",
      "2020-12-06 22:36:36,571 \n",
      "Results:\n",
      "- F1-score (micro) 0.7578\n",
      "- F1-score (macro) 0.7107\n",
      "\n",
      "By class:\n",
      "LOC        tp: 893 - fp: 268 - fn: 293 - precision: 0.7692 - recall: 0.7530 - f1-score: 0.7610\n",
      "MISC       tp: 193 - fp: 120 - fn: 167 - precision: 0.6166 - recall: 0.5361 - f1-score: 0.5736\n",
      "ORG        tp: 146 - fp: 76 - fn: 93 - precision: 0.6577 - recall: 0.6109 - f1-score: 0.6334\n",
      "PER        tp: 650 - fp: 122 - fn: 64 - precision: 0.8420 - recall: 0.9104 - f1-score: 0.8748\n",
      "2020-12-06 22:36:36,572 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.7578014898328973,\n",
       " 'dev_score_history': [0.735024048972453,\n",
       "  0.7414803559800304,\n",
       "  0.7701674277016742],\n",
       " 'train_loss_history': [6.715684566924821,\n",
       "  3.360003551084604,\n",
       "  2.7185269476762457],\n",
       " 'dev_loss_history': [2.9247255325317383,\n",
       "  2.4928743839263916,\n",
       "  2.0861685276031494]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner-lepetit',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=3,\n",
    "              checkpoint=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceTagger.load('resources/taggers/example-ner-lepetit/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example sentence\n",
    "sentence = Sentence('Emma Louise, habite au 26 rue Alexandre, 75005 Paris, France, née le 11/11/1993. Elle travaille chez Zenika et elle est joignable sur 06660006.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.6 ms, sys: 99.1 ms, total: 146 ms\n",
      "Wall time: 256 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict tags and print\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma <B-PER> Louise <E-PER> , habite au 26 rue <B-LOC> Alexandre <E-LOC> , 75005 Paris <S-LOC> , France <S-LOC> , née le 11 / 11 / 1993 . Elle travaille chez Zenika <S-ORG> et elle est joignable sur 06660006 .\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
