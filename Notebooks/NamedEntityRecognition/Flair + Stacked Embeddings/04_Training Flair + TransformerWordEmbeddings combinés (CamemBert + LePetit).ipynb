{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "import flair\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
    "from typing import List\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus  = flair.datasets.WIKINER_FRENCH().downsample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    TransformerWordEmbeddings('camembert-base'),\n",
    "    TransformerWordEmbeddings('illuin/lepetit')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize trainer\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 08:56:19,461 Reading data from /Users/amalbedoui/.flair/datasets/wikiner_french\n",
      "2020-12-07 08:56:19,462 Train: /Users/amalbedoui/.flair/datasets/wikiner_french/aij-wikiner-fr-wp3.train\n",
      "2020-12-07 08:56:19,462 Dev: None\n",
      "2020-12-07 08:56:19,463 Test: None\n",
      "2020-12-07 08:56:35,289 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,293 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): TransformerWordEmbeddings(\n",
      "      (model): CamembertModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(32005, 256, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 256, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 256)\n",
      "          (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-07 08:56:35,295 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,296 Corpus: \"Corpus: 10713 train + 1190 dev + 1323 test sentences\"\n",
      "2020-12-07 08:56:35,296 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,297 Parameters:\n",
      "2020-12-07 08:56:35,298  - learning_rate: \"0.1\"\n",
      "2020-12-07 08:56:35,299  - mini_batch_size: \"32\"\n",
      "2020-12-07 08:56:35,299  - patience: \"3\"\n",
      "2020-12-07 08:56:35,300  - anneal_factor: \"0.5\"\n",
      "2020-12-07 08:56:35,301  - max_epochs: \"3\"\n",
      "2020-12-07 08:56:35,301  - shuffle: \"True\"\n",
      "2020-12-07 08:56:35,302  - train_with_dev: \"False\"\n",
      "2020-12-07 08:56:35,303  - batch_growth_annealing: \"False\"\n",
      "2020-12-07 08:56:35,303 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,304 Model training base path: \"resources/taggers/example-ner-combined-cam+lep\"\n",
      "2020-12-07 08:56:35,304 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,305 Device: cpu\n",
      "2020-12-07 08:56:35,306 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 08:56:35,307 Embeddings storage mode: cpu\n",
      "2020-12-07 08:56:35,314 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 09:00:47,564 epoch 1 - iter 33/335 - loss 20.69014318 - samples/sec: 4.20 - lr: 0.100000\n",
      "2020-12-07 09:04:35,236 epoch 1 - iter 66/335 - loss 14.45039527 - samples/sec: 4.65 - lr: 0.100000\n",
      "2020-12-07 09:08:46,290 epoch 1 - iter 99/335 - loss 11.81696002 - samples/sec: 4.22 - lr: 0.100000\n",
      "2020-12-07 09:13:06,618 epoch 1 - iter 132/335 - loss 10.25834715 - samples/sec: 4.07 - lr: 0.100000\n",
      "2020-12-07 09:17:29,499 epoch 1 - iter 165/335 - loss 9.22757678 - samples/sec: 4.03 - lr: 0.100000\n",
      "2020-12-07 09:22:00,674 epoch 1 - iter 198/335 - loss 8.45175765 - samples/sec: 3.90 - lr: 0.100000\n",
      "2020-12-07 09:26:15,207 epoch 1 - iter 231/335 - loss 7.86870665 - samples/sec: 4.16 - lr: 0.100000\n",
      "2020-12-07 09:30:38,408 epoch 1 - iter 264/335 - loss 7.39674043 - samples/sec: 4.02 - lr: 0.100000\n",
      "2020-12-07 09:34:59,821 epoch 1 - iter 297/335 - loss 7.04839800 - samples/sec: 4.05 - lr: 0.100000\n",
      "2020-12-07 09:39:15,585 epoch 1 - iter 330/335 - loss 6.72193103 - samples/sec: 4.14 - lr: 0.100000\n",
      "2020-12-07 09:39:51,021 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 09:39:51,022 EPOCH 1 done: loss 6.6771 - lr 0.1000000\n",
      "2020-12-07 09:43:23,974 DEV : loss 2.668398141860962 - score 0.7587\n",
      "2020-12-07 09:43:25,293 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 09:43:26,590 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 09:47:46,254 epoch 2 - iter 33/335 - loss 3.61921032 - samples/sec: 4.08 - lr: 0.100000\n",
      "2020-12-07 09:52:02,565 epoch 2 - iter 66/335 - loss 3.50383260 - samples/sec: 4.13 - lr: 0.100000\n",
      "2020-12-07 09:56:00,141 epoch 2 - iter 99/335 - loss 3.38592767 - samples/sec: 4.46 - lr: 0.100000\n",
      "2020-12-07 10:00:15,873 epoch 2 - iter 132/335 - loss 3.31112046 - samples/sec: 4.14 - lr: 0.100000\n",
      "2020-12-07 10:04:33,158 epoch 2 - iter 165/335 - loss 3.23864062 - samples/sec: 4.12 - lr: 0.100000\n",
      "2020-12-07 10:08:36,589 epoch 2 - iter 198/335 - loss 3.17694798 - samples/sec: 4.35 - lr: 0.100000\n",
      "2020-12-07 10:12:39,933 epoch 2 - iter 231/335 - loss 3.11964828 - samples/sec: 4.35 - lr: 0.100000\n",
      "2020-12-07 10:16:59,037 epoch 2 - iter 264/335 - loss 3.08909295 - samples/sec: 4.08 - lr: 0.100000\n",
      "2020-12-07 10:21:01,199 epoch 2 - iter 297/335 - loss 3.06154589 - samples/sec: 4.37 - lr: 0.100000\n",
      "2020-12-07 10:25:14,464 epoch 2 - iter 330/335 - loss 3.04091507 - samples/sec: 4.18 - lr: 0.100000\n",
      "2020-12-07 10:25:56,065 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 10:25:56,067 EPOCH 2 done: loss 3.0292 - lr 0.1000000\n",
      "2020-12-07 10:29:25,229 DEV : loss 1.9775595664978027 - score 0.8014\n",
      "2020-12-07 10:29:26,228 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 10:29:27,454 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 10:33:48,810 epoch 3 - iter 33/335 - loss 2.30955320 - samples/sec: 4.06 - lr: 0.100000\n",
      "2020-12-07 10:38:15,983 epoch 3 - iter 66/335 - loss 2.48145960 - samples/sec: 3.96 - lr: 0.100000\n",
      "2020-12-07 10:42:28,500 epoch 3 - iter 99/335 - loss 2.47724800 - samples/sec: 4.19 - lr: 0.100000\n",
      "2020-12-07 10:46:23,235 epoch 3 - iter 132/335 - loss 2.47032344 - samples/sec: 4.51 - lr: 0.100000\n",
      "2020-12-07 10:50:22,180 epoch 3 - iter 165/335 - loss 2.48537554 - samples/sec: 4.43 - lr: 0.100000\n",
      "2020-12-07 10:54:21,649 epoch 3 - iter 198/335 - loss 2.47389724 - samples/sec: 4.42 - lr: 0.100000\n",
      "2020-12-07 10:58:22,983 epoch 3 - iter 231/335 - loss 2.44080888 - samples/sec: 4.39 - lr: 0.100000\n",
      "2020-12-07 11:02:24,730 epoch 3 - iter 264/335 - loss 2.41131534 - samples/sec: 4.38 - lr: 0.100000\n",
      "2020-12-07 11:06:31,806 epoch 3 - iter 297/335 - loss 2.39407502 - samples/sec: 4.29 - lr: 0.100000\n",
      "2020-12-07 11:10:29,749 epoch 3 - iter 330/335 - loss 2.39345405 - samples/sec: 4.45 - lr: 0.100000\n",
      "2020-12-07 11:11:05,181 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 11:11:05,182 EPOCH 3 done: loss 2.3948 - lr 0.1000000\n",
      "2020-12-07 11:14:25,325 DEV : loss 1.743804931640625 - score 0.8147\n",
      "2020-12-07 11:14:26,261 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-12-07 11:14:27,985 ----------------------------------------------------------------------------------------------------\n",
      "2020-12-07 11:14:27,989 Testing using best model ...\n",
      "2020-12-07 11:14:27,993 loading file resources/taggers/example-ner-combined-cam+lep/best-model.pt\n",
      "2020-12-07 11:18:20,400 0.8231\t0.8213\t0.8222\n",
      "2020-12-07 11:18:20,404 \n",
      "Results:\n",
      "- F1-score (micro) 0.8222\n",
      "- F1-score (macro) 0.7799\n",
      "\n",
      "By class:\n",
      "LOC        tp: 1021 - fp: 214 - fn: 202 - precision: 0.8267 - recall: 0.8348 - f1-score: 0.8308\n",
      "MISC       tp: 262 - fp: 113 - fn: 145 - precision: 0.6987 - recall: 0.6437 - f1-score: 0.6701\n",
      "ORG        tp: 160 - fp: 56 - fn: 82 - precision: 0.7407 - recall: 0.6612 - f1-score: 0.6987\n",
      "PER        tp: 726 - fp: 83 - fn: 43 - precision: 0.8974 - recall: 0.9441 - f1-score: 0.9202\n",
      "2020-12-07 11:18:20,405 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.8222137983320699,\n",
       " 'dev_score_history': [0.758683314415437,\n",
       "  0.8013856812933026,\n",
       "  0.8147465437788018],\n",
       " 'train_loss_history': [6.677065119814517,\n",
       "  3.0292117912377883,\n",
       "  2.394785000672981],\n",
       " 'dev_loss_history': [2.668398141860962,\n",
       "  1.9775595664978027,\n",
       "  1.743804931640625]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start training\n",
    "trainer.train('resources/taggers/example-ner-combined-cam+lep',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=3,\n",
    "              checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceTagger.load('resources/taggers/example-ner-combined-cam+lep/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example sentence\n",
    "sentence = Sentence('Emma Louise, habite au 26 rue Alexandre, 75005 Paris, France, née le 11/11/1993. Elle travaille chez Zenika et elle est joignable sur 06660006.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 12.8 ms, total: 222 ms\n",
      "Wall time: 218 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict tags and print\n",
    "model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma <B-PER> Louise <E-PER> , habite au 26 rue <B-LOC> Alexandre <E-LOC> , 75005 Paris <S-LOC> , France <S-LOC> , née le 11 / 11 / 1993 . Elle travaille chez Zenika <S-ORG> et elle est joignable sur 06660006 .\n"
     ]
    }
   ],
   "source": [
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
